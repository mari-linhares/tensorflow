{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hcD2nPQvPOFM"
   },
   "source": [
    "##### Copyright 2018 The TensorFlow Authors.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\").\n",
    "\n",
    "# Text Generation using a RNN\n",
    "\n",
    "<table class=\"tfo-notebook-buttons\" align=\"left\"><td>\n",
    "<a target=\"_blank\"  href=\"https://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/contrib/eager/python/examples/generative_examples/text_generation.ipynb\">\n",
    "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>  \n",
    "</td><td>\n",
    "<a target=\"_blank\"  href=\"https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/eager/python/examples/generative_examples/text_generation.ipynb\"><img width=32px src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on Github</a></td></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BwpJ5IffzRG6"
   },
   "source": [
    "This notebook demonstrates how to generate text using an character based RNN using [tf.keras](https://www.tensorflow.org/programmers_guide/keras) and [eager execution](https://www.tensorflow.org/programmers_guide/eager). Here, we show a lower-level implementation that's useful to understand as prework before diving in to deeper examples in a similar, like [Neural Machine Translation with Attention](https://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/contrib/eager/python/examples/nmt_with_attention/nmt_with_attention.ipynb). Also, if you like, you can write a similar [model](https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/8.1-text-generation-with-lstm.ipynb) using less code.\n",
    "\n",
    "This is an end-to-end example. We'll download a dataset of Shakespeare's writing, a collection of plays, borrowed from Andrej Karpathy's excellent [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/), then given a sequence of characters we'll train a RNN model to predict the next character and use it to generate similar text.\n",
    "\n",
    "Here is a output sample (with start string='w') after training a single GRU layer for 30 epochs with the default settings below:\n",
    "\n",
    "```\n",
    "were to the death of him\n",
    "And nothing of the field in the view of hell,\n",
    "When I said, banish him, I will not burn thee that would live.\n",
    "\n",
    "HENRY BOLINGBROKE:\n",
    "My gracious uncle--\n",
    "\n",
    "DUKE OF YORK:\n",
    "As much disgraced to the court, the gods them speak,\n",
    "And now in peace himself excuse thee in the world.\n",
    "\n",
    "HORTENSIO:\n",
    "Madam, 'tis not the cause of the counterfeit of the earth,\n",
    "And leave me to the sun that set them on the earth\n",
    "And leave the world and are revenged for thee.\n",
    "\n",
    "GLOUCESTER:\n",
    "I would they were talking with the very name of means\n",
    "To make a puppet of a guest, and therefore, good Grumio,\n",
    "Nor arm'd to prison, o' the clouds, of the whole field,\n",
    "With the admire\n",
    "With the feeding of thy chair, and we have heard it so,\n",
    "I thank you, sir, he is a visor friendship with your silly your bed.\n",
    "\n",
    "SAMPSON:\n",
    "I do desire to live, I pray: some stand of the minds, make thee remedies\n",
    "With the enemies of my soul.\n",
    "\n",
    "MENENIUS:\n",
    "I'll keep the cause of my mistress.\n",
    "\n",
    "POLIXENES:\n",
    "My brother Marcius!\n",
    "\n",
    "Second Servant:\n",
    "Will't ple\n",
    "```\n",
    "\n",
    "Of course, while some of the sentences are grammatical, most do not make sense. But, consider:\n",
    "\n",
    "* Our model is character based (when we began training, it did not yet know how to spell a valid English word, or that words were even a unit of text).\n",
    "\n",
    "* The structure of the output resembles a play (blocks begin with a speaker name, in all caps similar to the original text). Sentences generally end with a period. If you look at the text from a distance (or don't read the individual words too closely, it appears as if it's an excerpt from a play).\n",
    "\n",
    "* As we'll show our model trains in small pieces of text (100 characters) and still can generate a long text that has coherent structure.\n",
    "\n",
    "As a next step, you can experiment training the model on a different dataset - any large text file (ASCII) will do, and you can modify a single line of code below to make that change. Have fun!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R3p22DBDsaCA"
   },
   "source": [
    "## Install unidecode library\n",
    "\n",
    "A helpful library to convert unicode to ASCII."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wZ6LOM12wKGH"
   },
   "outputs": [],
   "source": [
    "!pip install unidecode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WGyKZj3bzf9p"
   },
   "source": [
    "## Import tensorflow and enable eager execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yG_n40gFzf9s"
   },
   "outputs": [],
   "source": [
    "# Import TensorFlow >= 1.10 and enable eager execution\n",
    "import tensorflow as tf\n",
    "\n",
    "# Note: Once you enable eager execution, it cannot be disabled. \n",
    "tf.enable_eager_execution()\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import unidecode\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EHDoRoc5PKWz"
   },
   "source": [
    "## Download the dataset\n",
    "\n",
    "In this example, we will use the [shakespeare dataset](https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt). You can use any other dataset that you like.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pD_55cOxLkAb"
   },
   "outputs": [],
   "source": [
    "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UHjdCjDuSvX_"
   },
   "source": [
    "## Read the dataset\n",
    "\n",
    "First we'll have a look in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 1115394 characters\n"
     ]
    }
   ],
   "source": [
    "text = unidecode.unidecode(open(path_to_file).read())\n",
    "# length of text is the number of characters in it\n",
    "print ('Length of text: {} characters'.format(len(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# first 1000 characters in text\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 unique characters\n"
     ]
    }
   ],
   "source": [
    "# unique contains all the unique characters in the file\n",
    "unique = sorted(set(text))\n",
    "print ('{} unique characters'.format(len(unique)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LFjSVAlWzf-N"
   },
   "source": [
    "## Creating the input and output tensors\n",
    "\n",
    "Our model cannot understand strings only numbers, we need to map the string representation to a numerical representation. There are a couple of choices we need to make, per instance, to choose a character based model or a word based model, in practice for tasks involving text is a good idea to spend sometime thinking about the best representation for your problem.\n",
    "\n",
    "In this example we'll first map each character to a number, then we'll vectorize the number representation through an [embedding layer](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding), which will be described later in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IalZLbvOzf-F"
   },
   "outputs": [],
   "source": [
    "# creating a mapping from unique characters to indices\n",
    "char2idx = {u:i for i, u in enumerate(unique)}\n",
    "idx2char = {i:u for i, u in enumerate(unique)}\n",
    "\n",
    "text_as_int = [char2idx[c] for c in text]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have an integer representation for each character. Notice that we mapped the character as indexes from 0 to len(unique)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'\\n'   --->    0\t' '    --->    1\t'!'    --->    2\t'$'    --->    3\t'&'    --->    4\t\"'\"    --->    5\t','    --->    6\t'-'    --->    7\t'.'    --->    8\t'3'    --->    9\t':'    --->   10\t';'    --->   11\t'?'    --->   12\t'A'    --->   13\t'B'    --->   14\t'C'    --->   15\t'D'    --->   16\t'E'    --->   17\t'F'    --->   18\t'G'    --->   19\t'H'    --->   20\t'I'    --->   21\t'J'    --->   22\t'K'    --->   23\t'L'    --->   24\t'M'    --->   25\t'N'    --->   26\t'O'    --->   27\t'P'    --->   28\t'Q'    --->   29\t'R'    --->   30\t'S'    --->   31\t'T'    --->   32\t'U'    --->   33\t'V'    --->   34\t'W'    --->   35\t'X'    --->   36\t'Y'    --->   37\t'Z'    --->   38\t'a'    --->   39\t'b'    --->   40\t'c'    --->   41\t'd'    --->   42\t'e'    --->   43\t'f'    --->   44\t'g'    --->   45\t'h'    --->   46\t'i'    --->   47\t'j'    --->   48\t'k'    --->   49\t'l'    --->   50\t'm'    --->   51\t'n'    --->   52\t'o'    --->   53\t'p'    --->   54\t'q'    --->   55\t'r'    --->   56\t's'    --->   57\t't'    --->   58\t'u'    --->   59\t'v'    --->   60\t'w'    --->   61\t'x'    --->   62\t'y'    --->   63\t'z'    --->   64\t"
     ]
    }
   ],
   "source": [
    "for char in char2idx:\n",
    "    print('{:6s} ---> {:4d}'.format(repr(char), char2idx[char]), end='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen ---- characters mapped to int ---- > [18, 47, 56, 57, 58, 1, 15, 47, 58, 47, 64, 43, 52]\n"
     ]
    }
   ],
   "source": [
    "# original first 13 characters and its mapped int version\n",
    "print ('{} ---- characters mapped to int ---- > {}'.format(text[:13], text_as_int[:13]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a character, or a sequence of characters, what is the most probable next character? This is the actual task we'll train the model to be able to perform.\n",
    "\n",
    "The model input vector will be a sequence of characters, and the expected output will be the respective following characters at each time step.\n",
    "\n",
    "![](input-output-vectors-rnn.png)\n",
    "\n",
    "TODO: Add diagram from https://docs.google.com/drawings/d/1MzyjFGelcJDSRftBxfIEUOW6cd2GA7BjkKPfM5OK8r4/edit?usp=sharing\n",
    "\n",
    "Since RNNs processes sequence input, in practice the input and expected output looks something more like:\n",
    "\n",
    "![](input-output-vectors-rnn-2.png)\n",
    "\n",
    "\n",
    "TODO: Add diagram from https://docs.google.com/drawings/d/1MzyjFGelcJDSRftBxfIEUOW6cd2GA7BjkKPfM5OK8r4/edit?usp=sharing\n",
    "\n",
    "As usual train, we'll give the data for the model in batches, so we'll create **max_length** chunks of input, where each input vector is all the characters in that chunk except the last and the target vector is all the characters in that chunk except the first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0UHJDA39zf-O"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11153, 100)\n",
      "(11153, 100)\n"
     ]
    }
   ],
   "source": [
    "# setting the maximum length sentence we want for a single input in characters\n",
    "max_length = 100\n",
    "\n",
    "# creating chunks of length == max_length\n",
    "input_text = []\n",
    "target_text = []\n",
    "\n",
    "for f in range(0, len(text_as_int)-max_length, max_length):\n",
    "    inputs = text_as_int[f : f + max_length]\n",
    "    targets = text_as_int[f + 1 : f + 1 + max_length]\n",
    "\n",
    "    input_text.append(inputs)\n",
    "    target_text.append(targets)\n",
    "    \n",
    "print (np.array(input_text).shape)\n",
    "print (np.array(target_text).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see we have 11153 examples of long sentences with around 100 characters. Let's print the first 10 inputs of the chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step    1, input: 'F' , expected output: 'i' \n",
      "Step    2, input: 'i' , expected output: 'r' \n",
      "Step    3, input: 'r' , expected output: 's' \n",
      "Step    4, input: 's' , expected output: 't' \n",
      "Step    5, input: 't' , expected output: ' ' \n",
      "Step    6, input: ' ' , expected output: 'C' \n",
      "Step    7, input: 'C' , expected output: 'i' \n",
      "Step    8, input: 'i' , expected output: 't' \n",
      "Step    9, input: 't' , expected output: 'i' \n",
      "Step   10, input: 'i' , expected output: 'z' \n"
     ]
    }
   ],
   "source": [
    "for i, (input_idx, target_idx) in enumerate(zip(input_text[0][:10], target_text[0][:10])):\n",
    "    print ('Step {:4d}, input: {:4s}, expected output: {:4s}'.format(i+1,\n",
    "                                                            repr(idx2char[input_idx]),\n",
    "                                                            repr(idx2char[target_idx])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MJdfPmdqzf-R"
   },
   "source": [
    "## Creating batches and shuffling them using tf.data\n",
    "\n",
    "We'll use [tf.data](https://www.tensorflow.org/guide/datasets) to feed the data to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p2pGotuNzf-S"
   },
   "outputs": [],
   "source": [
    "# batch size \n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# buffer size to shuffle the dataset\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_text, target_text)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m8gPwEjRzf-Z"
   },
   "source": [
    "## Implementing the model\n",
    "\n",
    "We use the Model Subclassing API which gives us full flexibility to create the model and change it however we like. We use 3 layers to define our model.\n",
    "\n",
    "* Embedding layer: a trainable lookup table that will map the numbers of each character to a high dimensional vector with **embedding_dim** dimensions;\n",
    "* GRU layer: a type of RNN (you can use an LSTM layer here) with layer size = **units**;\n",
    "* Fully connected layer with **vocab_size** cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P3KTiiInzf-a"
   },
   "outputs": [],
   "source": [
    "class Model(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, units):\n",
    "    super(Model, self).__init__()\n",
    "    self.units = units\n",
    "\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "    if tf.test.is_gpu_available():\n",
    "      self.gru = tf.keras.layers.CuDNNGRU(self.units, \n",
    "                                          return_sequences=True, \n",
    "                                          return_state=True, \n",
    "                                          recurrent_initializer='glorot_uniform')\n",
    "    else:\n",
    "      self.gru = tf.keras.layers.GRU(self.units, \n",
    "                                     return_sequences=True, \n",
    "                                     return_state=True, \n",
    "                                     recurrent_activation='sigmoid', \n",
    "                                     recurrent_initializer='glorot_uniform')\n",
    "\n",
    "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "  def call(self, x, hidden):\n",
    "    x = self.embedding(x)\n",
    "\n",
    "    # output at every time step\n",
    "    # output shape == (batch_size, max_length, hidden_size) \n",
    "    # states variable to preserve the state of the model\n",
    "    # states shape == (batch_size, hidden_size)\n",
    "    output, states = self.gru(x, initial_state=hidden)\n",
    "    \n",
    "    # reshaping the output so that we can pass it to the Dense layer\n",
    "    # after reshaping the shape is (batch_size * max_length, hidden_size)\n",
    "    output = tf.reshape(output, (-1, output.shape[2]))\n",
    "    \n",
    "    # The dense layer will output predictions for every time_steps(max_length)\n",
    "    # output shape after the dense layer == (max_length * batch_size, vocab_size)\n",
    "    x = self.fc(output)\n",
    "    \n",
    "    # states will be used to pass at every step to the model while training\n",
    "    return x, states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "trpqTWyvk0nr"
   },
   "source": [
    "## Instantiate the model and set the optimizer and the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7t2XrzEOzf-e"
   },
   "outputs": [],
   "source": [
    "# length of the vocabulary in chars\n",
    "vocab_size = len(unique)\n",
    "\n",
    "# the embedding dimension \n",
    "embedding_dim = 256\n",
    "\n",
    "# number of RNN (here GRU) units\n",
    "units = 1024\n",
    "\n",
    "model = Model(vocab_size, embedding_dim, units)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use [Adam optimizer](https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer) with default arguments and [softmax cross entropy](https://www.tensorflow.org/api_docs/python/tf/losses/sparse_softmax_cross_entropy) as the loss function. This loss function is applicable in this context since we're trying to predict the next character and the number of characters is a discrete number, similar to a classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dkjWIATszf-h"
   },
   "outputs": [],
   "source": [
    "# using adam optimizer with default arguments\n",
    "optimizer = tf.train.AdamOptimizer()\n",
    "\n",
    "# using sparse_softmax_cross_entropy so that we don't have to create one-hot vectors\n",
    "def loss_function(real, preds):\n",
    "    return tf.losses.sparse_softmax_cross_entropy(labels=real, logits=preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3K6s6F79P7za"
   },
   "source": [
    "## Checkpoints (Object-based saving)\n",
    "\n",
    "We'll use [tf.train.Checkpoint](https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint) to save the weights of the model after a couple of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oAGisDdfP9rL"
   },
   "outputs": [],
   "source": [
    "# directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "# checkpoint instance\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer, model=model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lPrP0XMUzf-p"
   },
   "source": [
    "## Train the model\n",
    "\n",
    "Here we will use a custom training loop with the help of GradientTape():\n",
    "\n",
    "* We initialize the hidden state of the model with zeros and shape == (batch_size, number of rnn units). We do this by calling the function defined while creating the model.\n",
    "\n",
    "* Next, we iterate over the dataset(batch by batch) and calculate the **predictions and the hidden states** associated with that input.\n",
    "\n",
    "* There are a lot of interesting things happening here.\n",
    "  * The model gets hidden state (initialized with 0), lets call that **H0** and the first batch of input, lets call that **I0**.\n",
    "  * The model then returns the predictions **P1** and **H1**.\n",
    "  * For the next batch of input, the model receives **I1** and **H1**.\n",
    "  * The interesting thing here is that we pass **H1** to the model with **I1** which is how the model learns. The context learned from batch to batch is contained in the **hidden state**.\n",
    "  * We continue doing this until the dataset is exhausted and then we start a new epoch and repeat this.\n",
    "\n",
    "* After calculating the predictions, we calculate the **loss** using the loss function defined above. Then we calculate the gradients of the loss with respect to the model variables(input)\n",
    "\n",
    "* Finally, we take a step in that direction with the help of the optimizer using the apply_gradients function.\n",
    "\n",
    "Below is a diagram with all the steps from text data to training for the word \"bye\".\n",
    "\n",
    "TODO: add diagram [here](https://docs.google.com/drawings/d/1Fine4lNwuU-7lDLCvagA-uZhwIlWKh9c-kgsUkCitxM/edit?usp=sharing)\n",
    "\n",
    "![complete-example.png](complete-example.png)\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "Note: If you are running this notebook in Colab which has a **Tesla K80 GPU** it takes about 23 seconds per epoch.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d4tSNwymzf-q"
   },
   "outputs": [],
   "source": [
    "# Training step\n",
    "\n",
    "EPOCHS = 30\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    \n",
    "    # initializing the hidden state at the start of every epoch\n",
    "    # initally hidden is None\n",
    "    hidden = model.reset_states()\n",
    "    \n",
    "    for (batch, (inp, target)) in enumerate(dataset):\n",
    "          with tf.GradientTape() as tape:\n",
    "              # feeding the hidden state back into the model\n",
    "              # This is the interesting step\n",
    "              predictions, hidden = model(inp, hidden)\n",
    "              # reshaping the target because that's how the \n",
    "              # loss function expects it\n",
    "              target = tf.reshape(target, (-1,))\n",
    "              loss = loss_function(target, predictions)\n",
    "              \n",
    "          grads = tape.gradient(loss, model.variables)\n",
    "          optimizer.apply_gradients(zip(grads, model.variables))\n",
    "\n",
    "          if batch % 100 == 0:\n",
    "              print ('Epoch {} Batch {} Loss {:.4f}'.format(epoch+1,\n",
    "                                                            batch,\n",
    "                                                            loss))\n",
    "    # saving (checkpoint) the model every 5 epochs\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "      checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "    print ('Epoch {} Loss {:.4f}'.format(epoch+1, loss))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "01AR9vpNQMFF"
   },
   "source": [
    "## Restore the latest checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tyvpYomYQQkF"
   },
   "outputs": [],
   "source": [
    "# restoring the latest checkpoint in checkpoint_dir\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DjGz1tDkzf-u"
   },
   "source": [
    "## Predicting using our trained model\n",
    "\n",
    "The below code block is used to generated the text\n",
    "\n",
    "* We start by choosing a start string and initializing the hidden state and setting the number of characters we want to generate.\n",
    "\n",
    "* We get predictions using the start string and the hidden state.\n",
    "\n",
    "* Then we use a multinomial distribution to calculate the index of the predicted character. **We use this predicted character as our next input to the model**\n",
    "\n",
    "* **The hidden state returned by the model is fed back into the model so that it now has more context rather than just one word.** After we predict the next word, the modified hidden states are again fed back into the model, which is how it learns as it gets more context from the previously predicted words.\n",
    "\n",
    "\n",
    "TODO: add diagram from [here](https://docs.google.com/drawings/d/1M-7fE94Ql707YtRDV6AeW0oSqUi9PT79g9IlzkS-Jkk/edit?usp=sharing)\n",
    "![](example-4.png)\n",
    "\n",
    "If you see the predictions, the model knows when to capitalize, make paragraphs and the text follows a shakespeare style of writing which is pretty awesome!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WvuwZBX5Ogfd"
   },
   "outputs": [],
   "source": [
    "# Evaluation step (generating text using the model learned)\n",
    "\n",
    "# number of characters to generate\n",
    "num_generate = 1000\n",
    "\n",
    "# You can change the start string to experiment\n",
    "start_string = 'Q'\n",
    "# converting our start string to numbers (vectorizing!) \n",
    "input_eval = [char2idx[s] for s in start_string]\n",
    "input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "# empty string to store our results\n",
    "text_generated = ''\n",
    "\n",
    "# low temperatures results in more predictable text.\n",
    "# higher temperatures results in more surprising text\n",
    "# experiment to find the best setting\n",
    "temperature = 1.0\n",
    "\n",
    "# hidden state shape == (batch_size, number of rnn units); here batch size == 1\n",
    "hidden = [tf.zeros((1, units))]\n",
    "for i in range(num_generate):\n",
    "    predictions, hidden = model(input_eval, hidden)\n",
    "\n",
    "    # using a multinomial distribution to predict the word returned by the model\n",
    "    predictions = predictions / temperature\n",
    "    predicted_id = tf.multinomial(tf.exp(predictions), num_samples=1)[0][0].numpy()\n",
    "    \n",
    "    # We pass the predicted word as the next input to the model\n",
    "    # along with the previous hidden state\n",
    "    input_eval = tf.expand_dims([predicted_id], 0)\n",
    "    \n",
    "    text_generated += idx2char[predicted_id]\n",
    "\n",
    "print (start_string + text_generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AM2Uma_-yVIq"
   },
   "source": [
    "## Next steps\n",
    "\n",
    "* Change the start string to a different character, or the start of a sentence.\n",
    "* Experiment with training on a different, or with different parameters. [Project  Gutenberg](http://www.gutenberg.org/ebooks/100), for example, contains a large collection of books.\n",
    "* Experiment with the temperature parameter.\n",
    "* Add another RNN layer.\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "text_generation.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
