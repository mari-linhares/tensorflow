# ResNet with Estimator API

> **NOTE:** This tutorial is intended for *advanced* users of TensorFlow
and assumes expertise and experience in machine learning.

## Introduction

In this tutorial we'll build a ResNet for CIFAR-10, which is a popular dataset
for image classification using the Estimators API. Here we'll not going into
details about the model it self, the biggest contribution of this tutorial is
a practical example of how to build a distributed model with TensorFlow.

Before get started check these links:
  * [Estimators](https://www.tensorflow.org/extend/estimators)
  * [Distributed Tensorflow](https://www.tensorflow.org/deploy/distributed)
  * [Convolution Neural Networks tutorial: Training a model using multiple gpu cards](https://www.tensorflow.org/tutorials/deep_cnn#training_a_model_using_multiple_gpu_cards)

## Overview

CIFAR-10 classification is a common benchmark problem in machine learning.  The
problem is to classify RGB 32x32 pixel images across 10 categories:
```
airplane, automobile, bird, cat, deer, dog, frog, horse, ship, and truck.
```

For more details refer to the [CIFAR-10 page](http://www.cs.toronto.edu/~kriz/cifar.html)
and a [Tech Report](http://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf)
by Alex Krizhevsky.

The model will be a ResNet as proposed in:
```
Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun
Deep Residual Learning for Image Recognition. arXiv:1512.03385
```

Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun
Deep Residual Learning for Image Recognition. arXiv:1512.03385

### Goals

1. Highlights a canonical organization for network architecture, training and
   evaluation using the Estimators API.
2. Provides a template for constructing larger and more sophisticated models.

The reason CIFAR-10 was selected was that it is complex enough to exercise
much of TensorFlow's ability to scale to large models. At the same time,
the model is small enough to train fast, which is ideal for trying out
new ideas and experimenting with new techniques.

### Highlights of the Tutorial

* Multi-gpu example
* Distributed example using Experiments
* Shows how to create your own Hook
* More details about the Dataset API
* How to generate TFRecord files

We hope that this tutorial provides a launch point for building general models
(specially ResNets) with the Estimators API that are distributed by default.

## Code Organization

The code for this tutorial resides in
[`models/tutorials/image/cifar10_estimator/`](https://www.tensorflow.org/code/tensorflow_models/tutorials/image/cifar10_estimator/).

File | Purpose
--- | ---
[`generate_cifar10_tfrecords.py`](https://www.tensorflow.org/code/tensorflow_models/tutorials/image/cifar10_estimator/generate_cifar10_tfrecords.py) | Generates TFRecords from the Python CIFAR-10 data.
[`cifar10.py`](https://www.tensorflow.org/code/tensorflow_models/tutorials/image/cifar10_estimator/cifar10.py) | Input function implementation, reads the TFRecords generated by `generate_cifar10_tfrecords.py`.
[`cifar10_model.py`](https://www.tensorflow.org/code/tensorflow_models/tutorials/image/cifar10_estimator/cifar10_model.py) | Builds the ResNet model.
[`cifar10_main.py`](https://www.tensorflow.org/code/tensorflow_models/tutorials/image/cifar10_estimator/cifar10_main.py) | Trains a CIFAR-10 model on a CPU, GPU, multiple GPUS and even in multiple machines.


## TFRecord

#TODO: get tfrecord explanation


## Estimators

Estimators are a high Level abstraction that support all the basic
operations you need on a Machine Learning model.

![](imgs/estimator.png)

In order to implement our own Estimator we basically need:
  * An [input function](https://www.tensorflow.org/get_started/input_fn):
    this is the input pipeline implementation, where you're going to
    process your data and return the features and labels that will be used
    for trainining, evaluation and prediction using the Estimator interface.
  * A [model function](https://www.tensorflow.org/extend/estimators#constructing_the_model_fn):
    where will actually define our model, and the training, evaluation and
    prediction operations.

For more about how to implement an Estimator check
[this tutorial](https://www.tensorflow.org/versions/master/api_docs/python/tf/estimator/Estimator).

Now let's have a look at the code!

### Input function

The input function will define our input pipeline implementation, and it's
basically a function that manipulates the data and returns the features and
labels that will be used by the estimator for training, evaluation, and
prediction.

An efficient and scalable way to implement your own input function is to use the
[Dataset API](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/docs_src/programmers_guide/datasets.md).

The Dataset API enables you to build complex input pipelines from simple,
reusable pieces, making it easy to deal with large amounts of data, different
data formats, and complicated transformations.

Here's an input function implementation using the Dataset API.

```python
# Gets TFRecord from disk and repeats dataset undefinitely.
dataset = tf.contrib.data.TFRecordDataset(filenames).repeat()

# Parse records.
dataset = dataset.map(self.parser, num_threads=batch_size,
                      output_buffer_size=2 * batch_size)

# Potentially shuffle records.
if self.subset == 'train':
  min_queue_examples = int(
      Cifar10DataSet.num_examples_per_epoch(self.subset) * 0.4)
  # Ensure that the capacity is sufficiently large to provide good random
  # shuffling.
  dataset = dataset.shuffle(buffer_size=min_queue_examples + 3 * batch_size)

# Batch it up.
dataset = dataset.batch(batch_size)
iterator = dataset.make_one_shot_iterator()
image_batch, label_batch = iterator.get_next()

return image_batch, label_batch
```

The Dataset API introduces two new abstractions to TensorFlow: **datasets**
and **iterators**.

* A Dataset can either be a source or a transformation:
  * Creating a source (e.g. Dataset.from_tensor_slices()) constructs a dataset
    from one or more tf.Tensor objects.
  * Applying a transformation constructs a dataset from one or more
    tf.contrib.data.Dataset objects.
    * Repeat: produce multiple epochs;
    * Shuffle: it maintains a fixed-size buffer and chooses the next element
      uniformly at random from that buffer;
    * Batch: constructs a dataset by stacking consecutive elements of another
      dataset into a single element;
    * Map: applies a function to each element in.

* A Iterator provides the main way to extract elements from a dataset.
  The Iterator.get_next() operation yields the next element of a Dataset, and
  typically acts as the interface between input pipeline code and your model.



### Model Definition

We'll define our model implementing a model function, where we'll also define
the operations used for training, evaluation and prediction. In this tutorial
we'll focus on the model itself, and we'll comment briefly about the
operations choosen, since you can easily learn more about the operations and
what they're doing in the TensorFlow documentation and other online materials.

Our model function definition looks like:

```python
def model_fn(features, labels, mode, params):
  # model and operations definition
  ...
  # estimator definition
  return EstimatorSpec(...)
```

## Training

Now we can just create the Estimator using the model function above,
and call the methods available on the Estimator interface.

```python
estimator.train(input_fn=train_input_fn)
```

```shell
INFO:tensorflow:loss = 0.691962, step = 101 (36.537 sec)
INFO:tensorflow:global_step/sec: 3.79198
INFO:tensorflow:loss = 0.637554, step = 201 (26.371 sec)
INFO:tensorflow:global_step/sec: 4.12
INFO:tensorflow:loss = 0.461921, step = 301 (24.272 sec)
INFO:tensorflow:global_step/sec: 4.23288
INFO:tensorflow:loss = 0.456651, step = 401 (23.625 sec)
INFO:tensorflow:global_step/sec: 4.18946
INFO:tensorflow:loss = 0.603483, step = 501 (23.869 sec)
INFO:tensorflow:global_step/sec: 4.07666
INFO:tensorflow:loss = 0.617782, step = 601 (24.530 sec)
....
INFO:tensorflow:loss = 0.696719, step = 1001 (24.596 sec)
INFO:tensorflow:global_step/sec: 4.03502
INFO:tensorflow:loss = 0.519887, step = 1101 (24.783 sec)
INFO:tensorflow:global_step/sec: 3.93356
INFO:tensorflow:loss = 0.579439, step = 1201 (25.422 sec)
INFO:tensorflow:global_step/sec: 3.87702
```

## Evaluation

```python
estimator.evaluate(input_fn=eval_input_fn)
```

```shell
INFO:tensorflow:Evaluation [1/100]
INFO:tensorflow:Evaluation [2/100]
INFO:tensorflow:Evaluation [3/100]
INFO:tensorflow:Evaluation [4/100]
INFO:tensorflow:Evaluation [5/100]
INFO:tensorflow:Evaluation [6/100]
INFO:tensorflow:Evaluation [7/100]
INFO:tensorflow:Evaluation [8/100]
INFO:tensorflow:Evaluation [9/100]
INFO:tensorflow:Evaluation [10/100]
...
INFO:tensorflow:Evaluation [100/100]
INFO:tensorflow:Finished evaluation at 2017-07-24-20:39:32
INFO:tensorflow:Saving dict for global step 6262: accuracy = 0.856875, global_step = 6262, loss = 0.374715
```

## Training and Evaluation in a Distributed Environment

Another great thing about Estimators is that they are built to be easily
distributed. In other to run the model on a distributed way using
data-parallelism you can just create an experiment. Experiments know how to
invoke train and eval in a sensible fashion for distributed training.

Below is the code to create and run an experiment.

```python
def get_experiment(estimator, train_input, eval_input):
  def _experiment_fn(run_config, hparams):
    """Creates experiment.

    Experiments perform training on several workers in parallel,
    in other words Experiments know how to invoke train and eval
    in a sensible fashion for distributed training.

    We first prepare an estimator, and bundle it
    together with input functions for training and evaluation
    then collect all that in an Experiment object.
    """
    del run_config, hparams  #unused args
    return tf.contrib.learn.Experiment(
        estimator,
        train_input_fn=train_input,
        eval_input_fn=eval_input
    )
  return _experiment_fn

# run training and evaluation using an Experiment
learn_runner.run(get_experiment(estimator, train_input, eval_input),
                 run_config=run_config)

```
#TODO: review and improve this explanation

### Input function

* Mention Dataset API

The input part of the model is built by the functions `inputs()` and
`distorted_inputs()` which read images from the CIFAR-10 binary data files.
These files contain fixed byte length records, so we use
@{tf.FixedLengthRecordReader}.
See @{$reading_data#reading-from-files$Reading Data} to
learn more about how the `Reader` class works.

The images are processed as follows:

*  They are cropped to 24 x 24 pixels, centrally for evaluation or
   @{tf.random_crop$randomly} for training.
*  They are @{tf.image.per_image_standardization$approximately whitened}
   to make the model insensitive to dynamic range.

For training, we additionally apply a series of random distortions to
artificially increase the data set size:

* @{tf.image.random_flip_left_right$Randomly flip} the image from left to right.
* Randomly distort the @{tf.image.random_brightness$image brightness}.
* Randomly distort the @{tf.image.random_contrast$image contrast}.

Please see the @{$python/image$Images} page for the list of
available distortions. We also attach an
@{tf.summary.image} to the images
so that we may visualize them in @{$summaries_and_tensorboard$TensorBoard}.
This is a good practice to verify that inputs are built correctly.

<div style="width:50%; margin:auto; margin-bottom:10px; margin-top:20px;">
  <img style="width:70%" src="https://www.tensorflow.org/images/cifar_image_summary.png">
</div>

Reading images from disk and distorting them can use a non-trivial amount of
processing time. To prevent these operations from slowing down training, we run
them inside 16 separate threads which continuously fill a TensorFlow
@{tf.train.shuffle_batch$queue}.

### Using TensorFlow for Visualization

* 
* 

## Distributed

* Talk about experiments and how we're running distributed
* https://stackoverflow.com/questions/41600321/distributed-tensorflow-the-difference-between-in-graph-replication-and-between
* https://www.tensorflow.org/deploy/distributed


## What to expect

* Results table

