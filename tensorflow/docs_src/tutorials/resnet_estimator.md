# A guide to Multi-gpu and Distributed TensorFlow with Estimators API (better name?)

> **NOTE:** This guide is intended for *advanced* users of TensorFlow
and assumes expertise and experience in machine learning.

## Introduction

In this guide we'll go through a full implementation of a ResNet using
the Estimators API to classify images from CIFAR-10 dataset. The implementation
uses TensorFlow best practices for performance and is ready to run on a CPU,
multiple GPUs, and also multiple hosts.

The focus is not the model itself, the biggest contribution of this guide is
a practical example of how to build a model that can run on multiple GPUs and
also on multiple hosts using the TensorFlow high level APIs, and a short demo
that shows some of what to expect when doing so.

We assume you're already familiar with:
  * [Basic Estimators](https://www.tensorflow.org/extend/estimators)
  * [Distributed Tensorflow concepts](https://www.tensorflow.org/deploy/distributed)
  * [Using GPUs guide](https://www.tensorflow.org/tutorials/using_gpu)

Also is recommended to check these materials:
  * [Training a model using multiple gpu cards](https://www.tensorflow.org/tutorials/deep_cnn#training_a_model_using_multiple_gpu_cards)
  * [Placing Variables and Operations on Devices](https://www.tensorflow.org/tutorials/deep_cnn#placing_variables_and_operations_on_devices)

## Dataset and Model Overview

CIFAR-10 classification is a common benchmark problem in machine learning.  The
problem is to classify RGB 32x32 pixel images across 10 categories:
```
airplane, automobile, bird, cat, deer, dog, frog, horse, ship, and truck.
```

For more details refer to the [CIFAR-10 page](http://www.cs.toronto.edu/~kriz/cifar.html)
and a [Tech Report](http://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf)
by Alex Krizhevsky.

The reason CIFAR-10 was selected was that it is complex enough to exercise
much of TensorFlow's ability to scale to large models. At the same time,
the model is small enough to train fast, which is ideal for trying out
new ideas and experimenting with new techniques.

The model is a ResNet as proposed in:
```
Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun
Deep Residual Learning for Image Recognition. arXiv:1512.03385
```

### Highlights of the Tutorial

* Complete code implementation that runs on local CPU, GPUs and on multiple hosts;
* Explanation about how to run distributed TensorFlow using Experiments;
* Full code example on how to create a Hook;
* Practical example of a input function built with the Dataset API;
* Practical example of how to generate TFRecord files.

We hope that this guide provides a launch point for building general models
with the Estimators API implementing support to multiple GPUs and multiple hosts.

## Code Organization

The code for this tutorial resides in
[`models/tutorials/image/cifar10_estimator/`](https://www.tensorflow.org/code/tensorflow_models/tutorials/image/cifar10_estimator/).

File | Purpose
--- | ---
[`generate_cifar10_tfrecords.py`](https://www.tensorflow.org/code/tensorflow_models/tutorials/image/cifar10_estimator/generate_cifar10_tfrecords.py) | Generates TFRecords from the Python CIFAR-10 data.
[`cifar10.py`](https://www.tensorflow.org/code/tensorflow_models/tutorials/image/cifar10_estimator/cifar10.py) | Input function implementation, reads the TFRecords generated by `generate_cifar10_tfrecords.py`.
[`cifar10_model.py`](https://www.tensorflow.org/code/tensorflow_models/tutorials/image/cifar10_estimator/cifar10_model.py) | Builds the ResNet model.
[`cifar10_main.py`](https://www.tensorflow.org/code/tensorflow_models/tutorials/image/cifar10_estimator/cifar10_main.py) | Trains a CIFAR-10 model on a CPU, GPU, multiple GPUS and even in multiple machines.
[`model_base.py`](https://www.tensorflow.org/code/tensorflow_models/tutorials/image/cifar10_estimator/model_base.py) | Base for a ResNet model.


## About the Implementation

Here are some specific details about the implementation that we think is worth sharing.

Before reading the following subsections check the arguments available for
[`cifar10_main.py`](https://www.tensorflow.org/code/tensorflow_models/tutorials/image/cifar10_estimator/cifar10_main.py)
described in the beginning of the file.

### Multiple GPU Implementation

During training, training variable values are updated using aggregated gradients and deltas.
In this implementation we're folling the `parameter_server` implementation. More details can be found at
[performance/performance_models](https://www.tensorflow.org/performance/performance_models#parameter_server_variables).

If you run `cifar10_main.py` with the `--is_cpu_ps=True` argument (default) the parameters will be saved on the
CPU otherwise they will be spread across the available GPUs based on their size, for load balancing.

The implementation is basically the same described at
[Training a model using multiple gpu cards](https://www.tensorflow.org/tutorials/deep_cnn#training_a_model_using_multiple_gpu_cards).

Here is a diagram of this model:

<div style="width:40%; margin:auto; margin-bottom:10px; margin-top:20px;">
  <img style="width:100%" src="https://www.tensorflow.org/images/Parallelism.png">
</div>

If using `--is_cpu_ps=False` the gradients will be everaged and updated at the `/gpu:0` not
in the CPU.

### Distributed Settings Terminology

First, is important to notice that when runing locally with multiple GPUs
we're doing in-graph replication and synchronous training (using gradient averaging).

When running on distributed settings we're doint between-graph replication and we
can choose between asynchronous training and synchronous training.
We recommend doing synchronous training, for this we'll use [tf.train.SyncReplicasOptimizer](https://www.tensorflow.org/api_docs/python/tf/train/SyncReplicasOptimizer) as the model optimizer.

So in summary we're using between-graph replication for distributed training, 
and in-graph replication for using multiple GPUs in each worker.

Read [deploy/distributed/replicated_training](https://www.tensorflow.org/deploy/distributed#replicated_training)
for details about the terminology used.

### Distributed TensorFlow with Experiments

Estimators are scalable and distributed by design. In order to run the model
on a distributed settings using data-parallelism you can use an Experiment.
Experiments know how to invoke train and eval in a sensible fashion for
distributed training.

This is all the code used to create an Experiment at
[`cifar10_main.py`](https://www.tensorflow.org/code/tensorflow_models/tutorials/image/cifar10_estimator/cifar10_main.p).
```python
def get_experiment_fn(train_input_fn, eval_input_fn, train_steps, eval_steps,
                      train_hooks):
  """Returns an Experiment function."""
  def _experiment_fn(run_config, hparams):
    """Returns an Experiment."""
    del hparams  # Unused arg.
    # Create estimator.
    classifier = tf.estimator.Estimator(model_fn=_resnet_model_fn,
                                        config=run_config)
    # Create experiment.
    experiment = tf.contrib.learn.Experiment(
        classifier,
        train_input_fn=train_input_fn,
        eval_input_fn=eval_input_fn,
        train_steps=train_steps,
        eval_steps=eval_steps)
    # Adding hooks to be used by the estimator on training mode.
    experiment.extend_train_hooks(train_hooks)
    return experiment
  return _experiment_fn
```

Considering that you already have multiple hosts configured and an Estimator model using Experiments
you'll only need a `TF_CONFIG` environment variable setted up properly on each host.
You can set up the hosts manually or check [tensorflow/ecosystem](https://github.com/tensorflow/ecosystem)
for instructions about how to set up a Cluster.

The `TF_CONFIG` will be used by the [`RunConfig`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/learn/python/learn/estimators/run_config.py)
to know the existing hosts and their task: `master`, `ps` or `worker`.

Here's a `TF_CONFIG` example.

```python
cluster = {'master': ['master-ip:8000'],
           'ps': ['ps-ip:8000'],
           'worker': ['worker-ip:8000']}

TF_CONFIG = json.dumps(
  {'cluster': cluster,
   'task': {'type': master, 'index': 0},
   'model_dir': 'gs://<bucket_path>/<dir_path>',
   'environment': 'cloud'
  })
```

*Cluster*

A cluster spec, which is basically a dictionary that describes all of the tasks in the cluster. More about it at [tensorflow/deploy/distributed](https://www.tensorflow.org/deploy/distributed).

In this cluster spec we are defining a cluster with 1 master, 1 ps and 1 worker.

* `ps`: stands for Parameter Server, it saves the parameters among all workers.
        All workers can read/write/update the parameters for model via ps.
        As some models are extremely large the parameters are shared among the ps (each ps stores a subset).

* `worker`: does the training.

* `master`: basically a special worker, it does training, but also restores and saves checkpoints and do evaluation.

*Task*

The Task defines what is the role of the current node, for the previous example the node is the master on index 0
of the cluster spec, the task will be different for each node. An `TF_CONFIG` example for a worker would be:

```python
cluster = {'master': ['master-ip:8000'],
           'ps': ['ps-ip:8000'],
           'worker': ['worker-ip:8000']}

TF_CONFIG = json.dumps(
  {'cluster': cluster,
   'task': {'type': worker, 'index': 0},
   'model_dir': 'gs://<bucket_path>/<dir_path>',
   'environment': 'cloud'
  })
```

*Model_dir*

This is the path where the master will save the checkpoints, graph and TensorBoard files.
For a multi host environment you may want to use a Distributed File System, Google Storage and DFS are supported.

*Environment*

By default the environment is *local*, for a distributed setting we need to change it to *cloud*.

## The Input Pipeline

We need to implement an input function to feed the features and labels
(in case of supervisoned learning) to the Estimator. We'll use the
[Dataset API](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/docs_src/programmers_guide/datasets.md)
to implement the input function and TFRecords for the file format.

### Generating TFRecords

A TFRecords file represents a sequence of (binary) strings. The format
is not random access, so it is suitable for streaming large amounts of data
but not suitable if fast sharding or other non-sequential access is desired.

To generate a TFRecord file you need to create a
[`tf.python_io.TFRecordWriter`](https://www.tensorflow.org/api_docs/python/tf/python_io/TFRecordWriter)
for each file you want to write:

```python
record_writer = tf.python_io.TFRecordWriter(output_path)
```

You write a series of `tf.Example` protos to the file. Each example contain
a dictionary of features. Each feature can be a `FloatList`, `Int64List` or
`ByteList`.

So, for example, you might encode a single numpy-array image as:
```python
image_str = image.tostring()
image_str = tf.train.BytesList(value=[image_str])
image_str = tf.train.Feature(bytes_list=image_str)

width = tf.train.Int64List(value=[image.shape[0]])
width = tf.train.Feature(int64_list=width)

height = tf.train.Int64List(value=[image.shape[1]])
height = tf.train.Feature(int64_list=height)

features = {
    'height': height,
    'width': width,
    'image_str': image_str}
features = tf.train.Features(feature=features)

example = tf.train.Example(features=features)
writer.write(example.SerializeToString())
```

Full examples for popular datasets are available:
  * CIFAR-10: [`generate_cifar10_tfrecords.py`](https://github.com/tensorflow/models/blob/master/tutorials/image/cifar10_estimator/generate_cifar10_tfrecords.py)
  * MNIST: [`tensorflow/examples/how_tos/reading_data/fully_connected_reader.py`](https://github.com/tensorflow/tensorflow/blob/r1.2/tensorflow/examples/how_tos/reading_data/fully_connected_reader.py)

### The Dataset API

An efficient and scalable way to implement your own input function is to use the
[Dataset API](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/docs_src/programmers_guide/datasets.md).

The Dataset API enables you to build complex input pipelines from simple,
reusable pieces, making it easy to deal with large amounts of data, different
data formats, and complicated transformations.

Here is the core of the input function defined at
[`cifar10.py`](https://www.tensorflow.org/code/tensorflow_models/tutorials/image/cifar10_estimator/cifar10.py)
using the Dataset API to read from a TFRecord file.

```python
# Gets TFRecord from disk and repeats dataset undefinitely.
dataset = tf.contrib.data.TFRecordDataset(filenames).repeat()

# Parse records.
dataset = dataset.map(self.parser, num_threads=batch_size,
                      output_buffer_size=2 * batch_size)

# Potentially shuffle records.
if self.subset == 'train':
  min_queue_examples = int(
      Cifar10DataSet.num_examples_per_epoch(self.subset) * 0.4)
  # Ensure that the capacity is sufficiently large to provide good random
  # shuffling.
  dataset = dataset.shuffle(buffer_size=min_queue_examples + 3 * batch_size)

# Batch it up.
dataset = dataset.batch(batch_size)
iterator = dataset.make_one_shot_iterator()
image_batch, label_batch = iterator.get_next()

return image_batch, label_batch
```

The Dataset API introduces two new abstractions to TensorFlow: **datasets**
and **iterators**.

* A Dataset can either be a source or a transformation:
  * Creating a source (e.g. Dataset.from_tensor_slices()) constructs a dataset
    from one or more tf.Tensor objects.
  * Applying a transformation constructs a dataset from one or more
    tf.contrib.data.Dataset objects.
    * Repeat: produce multiple epochs;
    * Shuffle: it maintains a fixed-size buffer and chooses the next element
      uniformly at random from that buffer;
    * Batch: constructs a dataset by stacking consecutive elements of another
      dataset into a single element;
    * Map: applies a function to each element in.

* A Iterator provides the main way to extract elements from a dataset.
  The Iterator.get_next() operation yields the next element of a Dataset, and
  typically acts as the interface between input pipeline code and your model.

## Running and Checking Results

We'll discuss a quick demo of this model running with multiple GPUs on local and distributed
environments. This demo is not a benchmark, it was made as a practical example of what to
expect when running models with multiple GPUs and multiple hosts.

We've trained the model locally with 1, 2, 4 and 8 GPUs, and on distributed settings
with 1 master with 4 gpus, 1 worker with 4 gpus and 1 ps with 1 CPU. These were manually
configured using only a `TF_CONFIG` to do this.

### Images/sec

We should expect an almost linear increase in the *images/sec* metric as we add more gpus
since we're increasing the batch size along with the number of GPUs used, and the reason
why this may not be linear is that the model is not big enough to have a great
improvement from using a lot of GPUs.

<div style="width:95%; margin:auto; margin-bottom:10px; margin-top:20px;">
  <img style="width:80%" src="../images/resnet_estimator_images_sec.png">
</div>

> **EXERCISE**: Add more layers to the model and see if you get closer to a linear increase along with
the number of GPUs.

### Accuracy

It's important to make sure we're still getting the approximately same accuracy while adding more GPUs.
This may need some hyperparameter tuning, we didn't try to tune the model, instead we increased the
learning rate accordingly to the batch size (number of gpus) since larger mini-batches reduces the
variance of your stochastic gradient updates (considering that we're averaging the gradients in the mini-batch),
and this allows bigger step sizes.

Number of GPUs | Distributed | Best accuracy |
-------------- | ----------- | ------------- |
1              |     No      |     0.9297    |
2              |     No      |     0.9297    |
4              |     No      |     0.9121    |
8              |     No      |     0.9127    |
8              |     Yes     |     0.9174    |

Accuracy over time using TensorBoard for visualization:

<div style="width:95%; margin:auto; margin-bottom:10px; margin-top:20px;">
  <img style="width:80%" src="../images/resnet_estimator_accuracy.png">
</div>

> **EXERCISE**: Play with the hyperparameters trying to tune the model.

* @monteirom: verify these values;
An example why changing hyperparameters is important is that we tried to run this model with
initial learning rate of 0.1 with 8 gpus and weight decay of 1e-4, and the maximum accuracy we
got was about 85%, against 91% with initial learning rate of 0.8 and weight decay of 2e-4.
These small tunings can make a big difference while scaling your model.

### Global_step/sec

We should expect to see similar `global_step/sec` for each execution and slightly
smaller values when adding more gpus because of the overhead of dealing with multiple devices/hosts.

*global_step/sec* over time using TensorBoard for visualization:

<div style="width:95%; margin:auto; margin-bottom:10px; margin-top:20px;">
  <img style="width:80%" src="../images/resnet_estimator_global_step.png">
</div>

Even though we're not getting the exact same `global_step/sec` we should see a speed up
when adding GPUs since we need to train for less steps to train for the same number of epochs.

<div style="width:95%; margin:auto; margin-bottom:10px; margin-top:20px;">
  <img style="width:80%" src="../images/resnet_estimator_speedup.png">
</div>

> **EXERCISE**: Run this model with a constant batch size and with different number of GPUs
(e.g. 1, 2 and 4 gpus) you should the global_step/sec increasing as you add GPUs.
This also means the model should train faster as you add GPUs. This approach is not as
scalable as defining a constant batch size per GPU since at some point the batch size will
get very small for each GPU.

### Reproducing this Experiment

For local settings:

```shell
# 1 GPU
python cifar10_main.py --data_dir=gs://path/to/cifar10/tfrecords \
                       --num_gpus=1 \
                       --train_batch_size=128 \
                       --model_dir=/tmp/1gpu \
                       --train_steps=80000 \
                       --run_experiment=True \
                       --learning_rate=0.1 \
                       --eval_batch_size=200
# 2 GPUs
python cifar10_main.py --data_dir=gs://path/to/cifar10/tfrecords \
                       --num_gpus=2 \
                       --train_batch_size=256 \
                       --model_dir=/tmp/2gpus \
                       --train_steps=40000 \
                       --run_experiment=True \
                       --learning_rate=0.1 \
                       --eval_batch_size=200

# 4 GPUs
python cifar10_main.py --data_dir=gs://path/to/cifar10/tfrecords \
                       --num_gpus=4 \
                       --train_batch_size=512 \
                       --model_dir=/tmp/4gpus \
                       --train_steps=20000 \
                       --run_experiment=True \
                       --learning_rate=0.5 \
                       --eval_batch_size=200

# 8 GPUs
python cifar10_main.py --data_dir=gs://path/to/cifar10/tfrecords \
                       --num_gpus=8 \
                       --train_batch_size=1024 \
                       --model_dir=/tmp/8gpus \
                       --train_steps=10000 \
                       --run_experiment=True \
                       --learning_rate=0.8 \
                       --eval_batch_size=200
```

For distributed settings:
```shell
# Master
TF_CONFIG='{"environment": "cloud", "cluster": {"master": ["master-ip:8000"], "ps": ["ps-ip:8000"], "master": ["worker-ip:8000"]}, "task": {"index": 0, "type": "master"}, "model_dir": "gs://path/to/model_dir"}' \
python cifar10_main.py --data_dir=gs://path/to/cifar10/tfrecords \
                       --num_gpus=4 \
                       --train_batch_size=512 \
                       --model_dir=gs://path/to/model_dir \
                       --sync=True \
                       --train_steps=10000 \
                       --run_experiment=True \
                       --learning_rate=0.8 \
                       --eval_batch_size=200 \
                       --num_workers=2

# Worker
TF_CONFIG='{"environment": "cloud", "cluster": {"master": ["master-ip:8000"], "ps": ["ps-ip:8000"], "master": ["worker-ip:8000"]}, "task": {"index": 0, "type": "worker"}, "model_dir": "gs://path/to/model_dir"}' \
python cifar10_main.py --data_dir=gs://path/to/cifar10/tfrecords \
                       --num_gpus=4 \
                       --train_batch_size=512 \
                       --model_dir=gs://path/to/model_dir \
                       --sync=True \
                       --train_steps=10000 \
                       --run_experiment=True \
                       --learning_rate=0.8 \
                       --eval_batch_size=200 \
                       --num_workers=2

# PS
TF_CONFIG='{"environment": "cloud", "cluster": {"master": ["master-ip:8000"], "ps": ["ps-ip:8000"], "master": ["worker-ip:8000"]}, "task": {"index": 0, "type": "worker"}, "model_dir": "gs://path/to/model_dir"}' \
python cifar10_main.py --model_dir=gs://path/to/model_dir --run_experiment=True
```

### About the Environment

For local training we used the following environment on Google Compute Engine:

* **Machine type**: n1-standard-32 (32 vCPUs, 120 GB memory)
* **CPU platform**: Intel Haswell
* **GPUs**: 8 x NVIDIA Tesla K80
* **OS**: Ubuntu 16.04 LTS
* **CUDA / cuDNN**: 8.0 / 6.0 (@tobyboyd: how to check CUDA version?)

For distributed training we used the following environment on Google Compute Engine:

For the Master and Worker:

* **Machine type**: n1-standard-32 (32 vCPUs, 120 GB memory)
* **CPU platform**: Intel Haswell
* **GPUs**: 8 x NVIDIA Tesla K80
* **OS**: Ubuntu 16.04 LTS
* **CUDA / cuDNN**: 8.0 / 6.0 (@tobyboyd: how to check CUDA version?)

For the PS:

* **Machine type**: n1-standard-16 (16 vCPUs, 60 GB memory)
* **CPU platform**: Intel Haswell
* **OS**: Ubuntu 16.04 LTS

### Methodology

In order to create results that are as repeatable as possible each test was run 3 times and
then the times were averaged together for the *images/sec* metric, for the accuracy graph
we used the best accuracy we found while training, and for the *global_step/sec* accuracy all
3 runs got very similar graphs over time, so we picked the runs that had the best accuracy as well.
We defined the batch size as a contast value of 128 per gpu, which means that if we're using 4 GPUs, per example, the total batch size will actually be 512. GPUs are run in their default state on the given platform.
For NVIDIA® Tesla® K80 this means leaving on [GPU Boost](https://devblogs.nvidia.com/parallelforall/increase-performance-gpu-boost-k80-autoboost/).

## Visualizing results with TensorFlow

When using Estimators you can use TensorBoard for visualization with no changes in the code,
and that's what we did for the accuracy and *global_step/sec* graphs available in this guide.

```shell
# Check TensorBoard during training or after it.
# Just point TensorBoard to the model_dir you chose.
$ tensorboard --log_dir="path/to/model_dir"
```
